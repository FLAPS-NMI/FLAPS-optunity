{
 "metadata": {
  "name": "",
  "signature": "sha256:622e6696890c825b50112f76f68f1961aab9e55f39eb0e5b47e7352d51d29fec"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "sklearn: SVM regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example we will show how to use Optunity to tune hyperparameters for support vector regression, more specifically:\n",
      "    \n",
      "* measure empirical improvements through nested cross-validation\n",
      "\n",
      "* optimizing hyperparameters for a given family of kernel functions\n",
      "\n",
      "* determining the optimal model without choosing the kernel in advance\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import itertools\n",
      "import optunity\n",
      "import optunity.metrics\n",
      "import sklearn.svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start by creating the data set. We use sklearn's diabetes data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_diabetes\n",
      "diabetes = load_diabetes()\n",
      "n = diabetes.data.shape[0]\n",
      "\n",
      "data = diabetes.data\n",
      "targets = diabetes.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Nested cross-validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nested cross-validation is used to estimate generalization performance of a full learning pipeline, which includes optimizing hyperparameters. We will use three folds in the outer loop. \n",
      "\n",
      "When using default hyperparameters, there is no need for an inner cross-validation procedure. However, if we want to include tuning in the learning pipeline, the inner loop is used to determine generalization performance with optimized hyperparameters.\n",
      "\n",
      "We start by measuring generalization performance with default hyperparameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we explicitly generate the outer_cv decorator so we can use it twice\n",
      "outer_cv = optunity.cross_validated(x=data, y=targets, num_folds=3)\n",
      "\n",
      "def compute_mse_standard(x_train, y_train, x_test, y_test):\n",
      "    \"\"\"Computes MSE of an SVR with RBF kernel and default hyperparameters.\n",
      "    \"\"\"\n",
      "    model = sklearn.svm.SVR().fit(x_train, y_train)\n",
      "    predictions = model.predict(x_test)\n",
      "    return optunity.metrics.mse(y_test, predictions)\n",
      "\n",
      "# wrap with outer cross-validation\n",
      "compute_mse_standard = outer_cv(compute_mse_standard)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`compute_mse_standard()` returns a three-fold cross-validation estimate of MSE for an SVR with default hyperparameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compute_mse_standard()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "6190.481497665955"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will create a function that returns MSE based on optimized hyperparameters, where we choose a polynomial kernel in advance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_mse_poly_tuned(x_train, y_train, x_test, y_test):\n",
      "    \"\"\"Computes MSE of an SVR with RBF kernel and optimized hyperparameters.\"\"\"\n",
      "\n",
      "    # define objective function for tuning\n",
      "    @optunity.cross_validated(x=x_train, y=y_train, num_iter=2, num_folds=5)\n",
      "    def tune_cv(x_train, y_train, x_test, y_test, C, degree, coef0):\n",
      "        model = sklearn.svm.SVR(C=C, degree=degree, coef0=coef0, kernel='poly').fit(x_train, y_train)\n",
      "        predictions = model.predict(x_test)\n",
      "        return optunity.metrics.mse(y_test, predictions)\n",
      "\n",
      "    # optimize parameters\n",
      "    optimal_pars, _, _ = optunity.minimize(tune_cv, 150, C=[1000, 20000], degree=[2, 5], coef0=[0, 1])\n",
      "    print(\"optimal hyperparameters: \" + str(optimal_pars))\n",
      "\n",
      "    tuned_model = sklearn.svm.SVR(kernel='poly', **optimal_pars).fit(x_train, y_train)\n",
      "    predictions = tuned_model.predict(x_test)\n",
      "    return optunity.metrics.mse(y_test, predictions)\n",
      "\n",
      "# wrap with outer cross-validation\n",
      "compute_mse_poly_tuned = outer_cv(compute_mse_poly_tuned)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`compute_mse_poly_tuned()` returns a three-fold cross-validation estimate of MSE for an SVR with RBF kernel with tuned hyperparameters $1000 < C < 20000$, $2 < degree < 5$ and $0 < coef0 < 1$ with a budget of 150 function evaluations. Each tuple of hyperparameters is evaluated using twice-iterated 5-fold cross-validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compute_mse_poly_tuned()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "optimal hyperparameters: {'C': 12078.673881034498, 'coef0': 0.5011052085197018, 'degree': 4.60890281463418}\n",
        "optimal hyperparameters: {'C': 14391.165364583334, 'coef0': 0.17313151041666666, 'degree': 2.35826171875}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "optimal hyperparameters: {'C': 11713.456382191061, 'coef0': 0.49836486667796476, 'degree': 4.616077904035152}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "3047.035965991627"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The polynomial kernel yields pretty good results when optimized, but maybe we can do even better with an RBF kernel."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_mse_rbf_tuned(x_train, y_train, x_test, y_test):\n",
      "    \"\"\"Computes MSE of an SVR with RBF kernel and optimized hyperparameters.\"\"\"\n",
      "\n",
      "    # define objective function for tuning\n",
      "    @optunity.cross_validated(x=x_train, y=y_train, num_iter=2, num_folds=5)\n",
      "    def tune_cv(x_train, y_train, x_test, y_test, C, gamma):\n",
      "        model = sklearn.svm.SVR(C=C, gamma=gamma).fit(x_train, y_train)\n",
      "        predictions = model.predict(x_test)\n",
      "        return optunity.metrics.mse(y_test, predictions)\n",
      "\n",
      "    # optimize parameters\n",
      "    optimal_pars, _, _ = optunity.minimize(tune_cv, 150, C=[1, 100], gamma=[0, 50])\n",
      "    print(\"optimal hyperparameters: \" + str(optimal_pars))\n",
      "\n",
      "    tuned_model = sklearn.svm.SVR(**optimal_pars).fit(x_train, y_train)\n",
      "    predictions = tuned_model.predict(x_test)\n",
      "    return optunity.metrics.mse(y_test, predictions)\n",
      "\n",
      "# wrap with outer cross-validation\n",
      "compute_mse_rbf_tuned = outer_cv(compute_mse_rbf_tuned)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`compute_mse_rbf_tuned()` returns a three-fold cross-validation estimate of MSE for an SVR with RBF kernel with tuned hyperparameters $1 < C < 100$ and $0 < \\gamma < 5$ with a budget of 150 function evaluations. Each tuple of hyperparameters is evaluated using twice-iterated 5-fold cross-validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compute_mse_rbf_tuned()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "optimal hyperparameters: {'C': 21.654003906250026, 'gamma': 16.536188056152554}\n",
        "optimal hyperparameters: {'C': 80.89867187499999, 'gamma': 3.2346692538501784}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "optimal hyperparameters: {'C': 19.35431640625002, 'gamma': 22.083848774716085}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "2990.8572696483493"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Woop! Seems like an RBF kernel is a good choice. An optimized RBF kernel leads to a 50% reduction in MSE compared to the default configuration."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Determining the kernel family during tuning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous part we've seen that the choice of kernel and its parameters significantly impact performance. However, testing every kernel family separately is cumbersome. It's better to let Optunity do the work for us.\n",
      "\n",
      "Optunity can optimize conditional search spaces, here the kernel family and depending on which family the hyperparameterization ($\\gamma$, degree, coef0, ...). We start by defining the search space (we will try the linear, polynomial and RBF kernel)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "space = {'kernel': {'linear': {'C': [0, 100]},\n",
      "                    'rbf': {'gamma': [0, 50], 'C': [1, 100]},\n",
      "                    'poly': {'degree': [2, 5], 'C': [1000, 20000], 'coef0': [0, 1]}\n",
      "                    }\n",
      "         }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we do nested cross-validation again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_mse_all_tuned(x_train, y_train, x_test, y_test):\n",
      "    \"\"\"Computes MSE of an SVR with RBF kernel and optimized hyperparameters.\"\"\"\n",
      "\n",
      "    # define objective function for tuning\n",
      "    @optunity.cross_validated(x=x_train, y=y_train, num_iter=2, num_folds=5)\n",
      "    def tune_cv(x_train, y_train, x_test, y_test, kernel, C, gamma, degree, coef0):\n",
      "        if kernel == 'linear':\n",
      "            model = sklearn.svm.SVR(kernel=kernel, C=C)\n",
      "        elif kernel == 'poly':\n",
      "            model = sklearn.svm.SVR(kernel=kernel, C=C, degree=degree, coef0=coef0)\n",
      "        elif kernel == 'rbf':\n",
      "            model = sklearn.svm.SVR(kernel=kernel, C=C, gamma=gamma)\n",
      "        else: \n",
      "            raise ArgumentError(\"Unknown kernel function: %s\" % kernel)\n",
      "        model.fit(x_train, y_train)\n",
      "\n",
      "        predictions = model.predict(x_test)\n",
      "        return optunity.metrics.mse(y_test, predictions)\n",
      "\n",
      "    # optimize parameters\n",
      "    optimal_pars, _, _ = optunity.minimize_structured(tune_cv, num_evals=150, search_space=space)\n",
      "    \n",
      "    # remove hyperparameters with None value from optimal pars\n",
      "    for k, v in optimal_pars.items():\n",
      "        if v is None: del optimal_pars[k]\n",
      "    print(\"optimal hyperparameters: \" + str(optimal_pars))\n",
      "    \n",
      "    tuned_model = sklearn.svm.SVR(**optimal_pars).fit(x_train, y_train)\n",
      "    predictions = tuned_model.predict(x_test)\n",
      "    return optunity.metrics.mse(y_test, predictions)\n",
      "\n",
      "# wrap with outer cross-validation\n",
      "compute_mse_all_tuned = outer_cv(compute_mse_all_tuned)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now the kernel family will be optimized along with its hyperparameterization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compute_mse_all_tuned()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "optimal hyperparameters: {'kernel': 'rbf', 'C': 33.70116043112164, 'gamma': 16.32317353448437}\n",
        "optimal hyperparameters: {'kernel': 'rbf', 'C': 58.11404170763237, 'gamma': 26.45349823062099}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "optimal hyperparameters: {'kernel': 'poly', 'C': 14964.421875843143, 'coef0': 0.5127175861493205, 'degree': 4.045210787998622}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "3107.625560844859"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like the RBF and polynomial kernel are competitive for this problem."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}