{
 "metadata": {
  "name": "",
  "signature": "sha256:f6eef60b835db38ade331fed4460fbaaa3a393c4493a7ec6962f21fce4b9fb12"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Basic: nested cross-validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will briefly illustrate how to use Optunity for nested cross-validation. \n",
      "\n",
      "Nested cross-validation is used to reliably estimate generalization performance of a learning pipeline (which may involve preprocessing, tuning, model selection, ...).\n",
      "Before starting this tutorial, we recommend making sure you are reliable with basic cross-validation in Optunity.\n",
      "\n",
      "We will use a scikit-learn SVM to illustrate the key concepts on the MNIST data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import optunity\n",
      "import optunity.cross_validation\n",
      "import optunity.metrics\n",
      "import numpy as np\n",
      "import sklearn.svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We load the digits data set and will construct models to distinguish digits 6 from and 8."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "n = digits.data.shape[0]\n",
      "\n",
      "positive_digit = 6\n",
      "negative_digit = 8\n",
      "\n",
      "positive_idx = [i for i in range(n) if digits.target[i] == positive_digit]\n",
      "negative_idx = [i for i in range(n) if digits.target[i] == negative_digit]\n",
      "\n",
      "# add some noise to the data to make it a little challenging\n",
      "original_data = digits.data[positive_idx + negative_idx, ...]\n",
      "data = original_data + 5 * np.random.randn(original_data.shape[0], original_data.shape[1])\n",
      "labels = [True] * len(positive_idx) + [False] * len(negative_idx)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The basic nested cross-validation scheme involves two cross-validation routines:\n",
      "    \n",
      "* outer cross-validation: to estimate the generalization performance of the learning pipeline. We will use 5folds.\n",
      "      \n",
      "* inner cross-validation: to use while optimizing hyperparameters. We will use twice iterated 10-fold cross-validation.\n",
      "\n",
      "Here, we have to take into account that we need to stratify the data based on the label, to ensure we don't run into situations where only one label is available in the train or testing splits. To do this, we use the `strata_by_labels` utility function.\n",
      "\n",
      "We will use an SVM with RBF kernel and optimize gamma on an exponential grid $10^-5 < \\gamma < 10^1$ and $0< C < 10$ on a linear grid."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# outer cross-validation to estimate performance of whole pipeline\n",
      "@optunity.cross_validated(x=data, y=labels, num_folds=5,\n",
      "                          strata=optunity.cross_validation.strata_by_labels(labels))\n",
      "def nested_cv(x_train, y_train, x_test, y_test):\n",
      "\n",
      "    # inner cross-validation to estimate performance of a set of hyperparameters\n",
      "    @optunity.cross_validated(x=x_train, y=y_train, num_folds=10, num_iter=2,\n",
      "                              strata=optunity.cross_validation.strata_by_labels(y_train))\n",
      "    def inner_cv(x_train, y_train, x_test, y_test, C, logGamma):\n",
      "        # note that the x_train, ... variables in this function are not the same\n",
      "        # as within nested_cv!\n",
      "        model = sklearn.svm.SVC(C=C, gamma=10 ** logGamma).fit(x_train, y_train)\n",
      "        predictions = model.decision_function(x_test)\n",
      "        return optunity.metrics.roc_auc(y_test, predictions)\n",
      "\n",
      "    hpars, info, _ = optunity.maximize(inner_cv, num_evals=100, \n",
      "                                    C=[0, 10], logGamma=[-5, 1])\n",
      "    print('')\n",
      "    print('Hyperparameters: ' + str(hpars))\n",
      "    print('Cross-validated AUROC after tuning: %1.3f' % info.optimum)\n",
      "    model = sklearn.svm.SVC(C=hpars['C'], gamma=10 ** hpars['logGamma']).fit(x_train, y_train)\n",
      "    predictions = model.decision_function(x_test)\n",
      "    return optunity.metrics.roc_auc(y_test, predictions)\n",
      "\n",
      "auc = nested_cv()\n",
      "print('')\n",
      "print('Nested AUROC: %1.3f' % auc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Hyperparameters: {'logGamma': -3.8679410473451057, 'C': 0.6162109374999996}\n",
        "Cross-validated AUROC after tuning: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Hyperparameters: {'logGamma': -4.535231399331072, 'C': 0.4839113474508706}\n",
        "Cross-validated AUROC after tuning: 0.999\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Hyperparameters: {'logGamma': -4.0821875, 'C': 1.5395986549905802}\n",
        "Cross-validated AUROC after tuning: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Hyperparameters: {'logGamma': -3.078125, 'C': 6.015625}\n",
        "Cross-validated AUROC after tuning: 1.000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Hyperparameters: {'logGamma': -4.630859375, 'C': 3.173828125}\n",
        "Cross-validated AUROC after tuning: 1.000\n",
        "\n",
        "Nested AUROC: 0.999\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to explicitly retain statistics from the inner cross-validation procedure, such as the ones we printed below, we can do so by returning tuples in the outer cross-validation and using the `identity` aggregator."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# outer cross-validation to estimate performance of whole pipeline\n",
      "@optunity.cross_validated(x=data, y=labels, num_folds=5,\n",
      "                          strata=optunity.cross_validation.strata_by_labels(labels),\n",
      "                          aggregator=optunity.cross_validation.identity)\n",
      "def nested_cv(x_train, y_train, x_test, y_test):\n",
      "\n",
      "    # inner cross-validation to estimate performance of a set of hyperparameters\n",
      "    @optunity.cross_validated(x=x_train, y=y_train, num_folds=10, num_iter=2,\n",
      "                              strata=optunity.cross_validation.strata_by_labels(y_train))\n",
      "    def inner_cv(x_train, y_train, x_test, y_test, C, logGamma):\n",
      "        # note that the x_train, ... variables in this function are not the same\n",
      "        # as within nested_cv!\n",
      "        model = sklearn.svm.SVC(C=C, gamma=10 ** logGamma).fit(x_train, y_train)\n",
      "        predictions = model.decision_function(x_test)\n",
      "        return optunity.metrics.roc_auc(y_test, predictions)\n",
      "\n",
      "    hpars, info, _ = optunity.maximize(inner_cv, num_evals=100, \n",
      "                                    C=[0, 10], logGamma=[-5, 1])\n",
      "    model = sklearn.svm.SVC(C=hpars['C'], gamma=10 ** hpars['logGamma']).fit(x_train, y_train)\n",
      "    predictions = model.decision_function(x_test)\n",
      "    \n",
      "    # return AUROC, optimized hyperparameters and AUROC during hyperparameter search\n",
      "    return optunity.metrics.roc_auc(y_test, predictions), hpars, info.optimum\n",
      "\n",
      "nested_cv_result = nested_cv()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then process the results like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aucs, hpars, optima = zip(*nested_cv_result)\n",
      "\n",
      "print(\"AUCs: \" + str(aucs))\n",
      "print('')\n",
      "print(\"hpars: \" + \"\\n\".join(map(str, hpars)))\n",
      "print('')\n",
      "print(\"optima: \" + str(optima))\n",
      "\n",
      "mean_auc = sum(aucs) / len(aucs)\n",
      "print('')\n",
      "print(\"Mean AUC %1.3f\" % mean_auc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AUCs: (0.9992063492063492, 1.0, 1.0, 0.9976190476190476, 0.9984126984126984)\n",
        "\n",
        "hpars: {'logGamma': -3.5753515625, 'C': 3.9048828125000004}\n",
        "{'logGamma': -2.6765234375, 'C': 6.9193359375000005}\n",
        "{'logGamma': -3.0538671875, 'C': 2.2935546875}\n",
        "{'logGamma': -3.593515625, 'C': 4.4136718749999995}\n",
        "{'logGamma': -3.337747403818736, 'C': 4.367953383541078}\n",
        "\n",
        "optima: (0.9995032051282051, 0.9985177917320774, 0.9994871794871795, 0.9995238095238095, 0.9995032051282051)\n",
        "\n",
        "Mean AUC 0.999\n"
       ]
      }
     ],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}